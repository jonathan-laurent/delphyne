"""
Script for running learning experiments with Delphyne.

!!! warning "Experimental"
    This is an experimental module that should evolve rapidly.

# Structure of learning experiment folders

```
<learning_experiment_name>
  - embeddings.cache.h5
  - iterations/<iteration_i>
    - feedback.yaml
    - train, test, analyze, summarize
    - learned
      - data
        - <QueryType>.tips.data.yaml
      - generated.demo.yaml
```

Iteration numbers start at 1.
"""

import random
from collections.abc import Sequence
from concurrent.futures import Future, ProcessPoolExecutor, as_completed
from dataclasses import dataclass, replace
from pathlib import Path
from typing import Any, Literal, Protocol, TypedDict, cast

import delphyne.analysis as an
import delphyne.core.demos as dm
import delphyne.core_and_base as dp
import delphyne.stdlib.answer_loaders as al
import delphyne.stdlib.commands as cmd
import delphyne.stdlib.execution_contexts as ec
import delphyne.stdlib.experiments.experiment_launcher as el
import delphyne.stdlib.feedback_processing as fp
import delphyne.stdlib.hindsight_feedback as hf
import delphyne.utils.typing as ty
from delphyne.utils.yaml import dump_yaml

GLOBAL_EMBEDDINGS_CACHE_FILE = ec.DEFAULT_GLOBAL_EMBEDDINGS_CACHE_FILE
ITERATIONS_DIR = "iterations"
FEEDBACK_FILE = "feedback.yaml"
TRAINING_EXP_DIR = "train"
TEST_EXP_DIR = "test"
ANALYZE_EXP_DIR = "analyze"
SUMMARIZE_EXP_DIR = "summarize"
LEARNED_DIR = "learned"
LEARNED_DATA_DIR = f"{LEARNED_DIR}/data"
LEARNED_DEMOS_FILE = f"{LEARNED_DIR}/generated.demo.yaml"
TIPS_FILE_SUFFIX = "_tips.data.yaml"  # preceded by the query type


#####
##### Definition of Learning Experiments
#####


type ProblemKind = Literal["train", "test"]
"""
Problem kind: "train" for training problems that are used to generate
feedback and "test" for test problems that are only used to evaluate
performance.
"""


class SolveProblemFn(Protocol):
    """
    Function protocol for solving train or test problems.
    """

    def __call__(
        self, problem_kind: ProblemKind, problem_name: str
    ) -> cmd.RunStrategyArgs: ...


class GenerateTipsFn(Protocol):
    """
    Function protocol for generating tips from feedback.
    """

    def __call__(
        self, feedback: "SerializedQueryFeedback"
    ) -> cmd.RunStrategyArgs: ...


class SummarizeTipsFn(Protocol):
    """
    Function protocol for summarizing tips.
    """

    def __call__(
        self, query_type: str, tips: "Sequence[Tip]"
    ) -> cmd.RunStrategyArgs: ...


@dataclass(kw_only=True)
class LearningExperiment:
    """
    A learning experiment, where new demonstrations and tips are
    iteratively generated by solving training problems.
    """

    context: ec.ExecutionContext
    training_problems: Sequence[str]
    testing_problems: Sequence[str]
    directory: Path
    solve_problem: SolveProblemFn
    generate_tips: GenerateTipsFn
    summarize_tips: SummarizeTipsFn
    feedback_filters: "FeedbackFilteringSettingsDict"
    enabled_feedback_nodes: Sequence[str]
    workers_setup: el.WorkersSetup[Any] | None = None

    def __post_init__(self):
        # Check that directory is absolute
        if not self.directory.is_absolute():
            raise ValueError(
                "LearningExperiment 'directory' must be an absolute path"
            )

    def run_cli(self):
        import fire  # type: ignore

        fire.Fire(LearningExperimentCLI(self))  # type: ignore

    @property
    def configs_context(self) -> "LearningExperimentContext":
        return LearningExperimentContext(
            directory=self.directory,
            solve_problem=self.solve_problem,
            generate_tips=self.generate_tips,
            summarize_tips=self.summarize_tips,
        )

    @property
    def query_types(self) -> Sequence[str]:
        """
        Return the list of all query types present in the feedback
        filtering settings.
        """
        return list(self.feedback_filters.keys())

    def solved_training_problems_at_iteration(
        self, iteration: int
    ) -> set[str]:
        """
        Return the set of all problems solved during a given iteration.
        """
        iteration_dir = _iteration_folder(self.directory, iteration)
        train_dir = iteration_dir / TRAINING_EXP_DIR
        summary_path = el.summary_file_path(train_dir)
        return _solved_problems_from_summary_file(summary_path)

    def training_problems_to_solve_for_iteration(
        self, iteration: int
    ) -> set[str]:
        """
        Return the set of problems to solve during a given iteration,
        which are the set of all problems that are unsolved so far.
        """
        prior_iterations = range(1, iteration)
        problems = set(self.training_problems)
        for it in prior_iterations:
            solved = self.solved_training_problems_at_iteration(it)
            problems.difference_update(solved)
        return problems

    def load_feedback_entries(
        self, iteration: int
    ) -> Sequence["SerializedQueryFeedback"]:
        """
        Load the proper feedback YAML file and return its content (in
        JSON).
        """
        return _load_feedback_entries(self.directory, iteration)

    def iteration_execution_context(self, iteration: int):
        previous_iters = range(1, iteration)
        new_data_dirs = [
            _iteration_folder(self.directory, it) / LEARNED_DATA_DIR
            for it in previous_iters
        ]
        new_demo_files = [
            _iteration_folder(self.directory, it) / LEARNED_DEMOS_FILE
            for it in previous_iters
        ]
        return replace(
            self.context,
            global_embeddings_cache_file=self.directory
            / GLOBAL_EMBEDDINGS_CACHE_FILE,
            data_dirs=[*self.context.data_dirs, *new_data_dirs],
            demo_files=[*self.context.demo_files, *new_demo_files],
        )

    ##### Obtaining Experiments #####

    def training_experiment(self, iteration: int):
        assert iteration >= 1
        to_solve = self.training_problems_to_solve_for_iteration(iteration)
        configs = [SolveProblemConfig("train", p) for p in to_solve]
        return el.Experiment[SolveProblemConfig](
            config_class=SolveProblemConfig,
            configs=configs,
            # Important: use a custom context that includes learned
            # facts and demonstrations.
            context=self.iteration_execution_context(iteration),
            output_dir=_iteration_folder(self.directory, iteration)
            / TRAINING_EXP_DIR,
            configs_context=self.configs_context,
            config_naming=lambda c, _: c.problem,
            workers_setup=self.workers_setup,
            export_raw_trace=True,
        )

    def testing_experiment(self, iteration: int):
        # We can have `iteration=0` here if we want to run tests before
        # any learning happened.
        assert iteration >= 0
        to_solve = self.testing_problems
        configs = [SolveProblemConfig("test", p) for p in to_solve]
        return el.Experiment[SolveProblemConfig](
            config_class=SolveProblemConfig,
            configs=configs,
            # Important: use a custom context that includes learned
            # facts and demonstrations.
            context=self.iteration_execution_context(iteration + 1),
            output_dir=_iteration_folder(self.directory, iteration)
            / TEST_EXP_DIR,
            configs_context=self.configs_context,
            config_naming=lambda c, _: c.problem,
            workers_setup=self.workers_setup,
            export_raw_trace=True,
        )

    def generate_tips_experiment(self, iteration: int):
        assert iteration >= 1
        feedback_entries = self.load_feedback_entries(iteration)
        configs = [
            GenerateTipsConfig(iteration=iteration, index=i)
            for i in range(len(feedback_entries))
        ]
        return el.Experiment[GenerateTipsConfig](
            config_class=GenerateTipsConfig,
            configs=configs,
            context=self.context,
            output_dir=_iteration_folder(self.directory, iteration)
            / ANALYZE_EXP_DIR,
            configs_context=self.configs_context,
            config_naming=lambda c, _: str(c.index),
            workers_setup=self.workers_setup,
            export_raw_trace=False,
        )

    def summarize_tips_experiment(self, iteration: int):
        assert iteration >= 1
        configs = [
            SummarizeTipsConfig(iteration=iteration, query_type=qt)
            for qt in self.query_types
        ]
        return el.Experiment[SummarizeTipsConfig](
            config_class=SummarizeTipsConfig,
            configs=configs,
            context=self.context,
            output_dir=_iteration_folder(self.directory, iteration)
            / SUMMARIZE_EXP_DIR,
            config_naming=lambda c, _: c.query_type,
            configs_context=self.configs_context,
            workers_setup=self.workers_setup,
            export_raw_trace=False,
        )

    ##### Processing Steps #####

    def generate_feedback_file(
        self, iteration: int, max_workers: int, show_progress: bool
    ) -> None:
        """
        Generate a feedback file for a given iteration by gathering
        feedback from all training experiment results and filtering them
        according to the configured settings.
        """
        # Gather feedback from all training experiment results
        iteration_dir = _iteration_folder(self.directory, iteration)
        train_dir = iteration_dir / TRAINING_EXP_DIR

        # Iterate through all problems that were attempted
        problems_to_solve = self.training_problems_to_solve_for_iteration(
            iteration
        )
        problems_list = list(problems_to_solve)

        all_feedback: list[QueryFeedback] = []
        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            futures: list[Future[Sequence[QueryFeedback]]] = []
            for problem_name in problems_list:
                command_file = el.result_file_path(train_dir, problem_name)
                assert command_file.exists()
                future = executor.submit(
                    _process_single_problem_feedback,
                    problem_name,
                    command_file,
                    self.context,
                    self.enabled_feedback_nodes,
                )
                futures.append(future)
            completed = 0
            for future in as_completed(futures):
                completed += 1
                if show_progress:
                    prog = f"{completed}/{len(problems_list)}"
                    print(
                        f"\rProcessing feedback: {prog} problems",
                        end="",
                        flush=True,
                    )
                problem_feedback = future.result()
                all_feedback.extend(problem_feedback)
            if show_progress:
                print()  # New line after progress

        rng = random.Random(42)  # Use a fixed seed for reproducibility
        filtered_feedback = filter_feedback(
            all_feedback, rng=rng, settings=self.feedback_filters
        )
        feedback_path = iteration_dir / FEEDBACK_FILE
        feedback_path.parent.mkdir(parents=True, exist_ok=True)
        feedback_content = dump_yaml(
            Sequence[QueryFeedback], filtered_feedback, exclude_defaults=False
        )
        feedback_path.write_text(feedback_content)

    def generate_learned_data(self, iteration: int) -> None:
        """
        Generate learned data (tips and demonstrations) for a given
        iteration. Tips are generated by copying the output of the
        summarization experiment. Demonstrations are generated from the
        feedback entries file.
        """
        iteration_dir = _iteration_folder(self.directory, iteration)
        feedback_entries = ty.pydantic_load(
            Sequence[QueryFeedback], self.load_feedback_entries(iteration)
        )

        demos_path = iteration_dir / LEARNED_DEMOS_FILE
        write_demonstration_file(feedback_entries, demos_path)

        for qt in self.query_types:
            result_file = el.result_file_path(
                iteration_dir / SUMMARIZE_EXP_DIR, qt
            )
            successes = al.load_success_values_from_command_file(
                result_file, Sequence[Tip]
            )
            tips: Sequence[Tip] = successes[0]
            data_dir = iteration_dir / LEARNED_DATA_DIR
            data_dir.mkdir(parents=True, exist_ok=True)
            tips_file = data_dir / f"{qt}{TIPS_FILE_SUFFIX}"
            tips_file_content = dump_yaml(Sequence[Tip], tips)
            tips_file.write_text(tips_file_content)

    ##### Analyze status #####

    def print_training_results(self, final_iteration: int):
        """
        For all iterations from 1 to `final_iteration`, show how many
        problems were solved during the iteration and the cumulative
        total. Also show those figures as percentages of the total number of
        training problems.
        """

        total_training = len(self.training_problems)
        cumulative_solved: set[str] = set()
        print("Training Results:")
        print("-" * 80)
        print(
            f"{'Iter':<6} {'Solved':<10} {'%':<8} {'Cumulative':<12} {'%':<8}"
        )
        print("-" * 80)
        for iteration in range(1, final_iteration + 1):
            iteration_dir = _iteration_folder(self.directory, iteration)
            train_dir = iteration_dir / TRAINING_EXP_DIR
            summary_path = el.summary_file_path(train_dir)
            if not summary_path.exists():
                print(
                    f"{iteration:<6} {'N/A':<10} {'N/A':<8} "
                    f"{'N/A':<12} {'N/A':<8}"
                )
                continue
            solved = _solved_problems_from_summary_file(summary_path)
            num_solved = len(solved)
            pct_solved = num_solved / total_training * 100
            cumulative_solved.update(solved)
            num_cumulative = len(cumulative_solved)
            pct_cumulative = num_cumulative / total_training * 100
            print(
                f"{iteration:<6} {num_solved:<10} {pct_solved:<8.1f} "
                f"{num_cumulative:<12} {pct_cumulative:<8.1f}"
            )
        print("-" * 80)

    def print_testing_results(self, final_iteration: int):
        """
        For all iterations from 0 to `final_iteration` and if testing
        was performed during this iteration, show how many problems were
        solved, in absolute and as a percentage of the total number of
        testing problems.
        """

        total_testing = len(self.testing_problems)
        print("Testing Results:")
        print("-" * 60)
        print(f"{'Iter':<6} {'Solved':<10} {'%':<8}")
        print("-" * 60)
        for iteration in range(0, final_iteration + 1):
            iteration_dir = _iteration_folder(self.directory, iteration)
            test_dir = iteration_dir / TEST_EXP_DIR
            summary_path = el.summary_file_path(test_dir)
            if not summary_path.exists():
                continue
            solved = _solved_problems_from_summary_file(summary_path)
            num_solved = len(solved)
            pct_solved = num_solved / total_testing * 100
            print(f"{iteration:<6} {num_solved:<10} {pct_solved:<8.1f}")
        print("-" * 60)


@dataclass
class LearningExperimentContext:
    """
    Subset of fields from `LearningExperiment` that are passed to
    configuration instantiation methods for context.
    """

    directory: Path
    solve_problem: SolveProblemFn
    generate_tips: GenerateTipsFn
    summarize_tips: SummarizeTipsFn


@dataclass
class SolveProblemConfig(el.ExperimentConfig):
    """
    Configuration for solving a train or test problem.
    """

    kind: ProblemKind
    problem: str

    def instantiate(self, context: object):
        assert isinstance(context, LearningExperimentContext)
        return context.solve_problem(self.kind, self.problem)


@dataclass
class GenerateTipsConfig(el.ExperimentConfig):
    """
    Configuration for generating tips from feedback.
    """

    iteration: int
    index: int

    def instantiate(self, context: object):
        assert isinstance(context, LearningExperimentContext)
        entries = _load_feedback_entries(context.directory, self.iteration)
        entry = entries[self.index]
        return context.generate_tips(entry)


@dataclass
class SummarizeTipsConfig(el.ExperimentConfig):
    """
    Configuration for summarizing tips.
    """

    iteration: int
    query_type: str

    def instantiate(self, context: object):
        assert isinstance(context, LearningExperimentContext)
        tips = _gather_tips(
            context.directory, self.iteration, query_type=self.query_type
        )
        return context.summarize_tips(self.query_type, tips)


def _iteration_folder(top_dir: Path, iteration: int) -> Path:
    return top_dir / ITERATIONS_DIR / str(iteration)


def _load_feedback_entries(
    top_dir: Path, iteration: int
) -> Sequence["SerializedQueryFeedback"]:
    """
    Load the proper feedback YAML file and return its content (in
    JSON).
    """
    import yaml

    iteration_dir = _iteration_folder(top_dir, iteration)
    feedback_path = iteration_dir / FEEDBACK_FILE
    with open(feedback_path, "r") as f:
        data = yaml.safe_load(f)
    assert isinstance(data, list)
    return cast(Any, data)


def _gather_tips(
    top_dir: Path, iteration: int, *, query_type: str
) -> Sequence["Tip"]:
    feedback_entries = _load_feedback_entries(top_dir, iteration)
    all_tips: list[Tip] = []
    for i in range(len(feedback_entries)):
        if feedback_entries[i]["query"]["name"] != query_type:
            continue
        iter_dir = _iteration_folder(top_dir, iteration)
        result_file = el.result_file_path(iter_dir / ANALYZE_EXP_DIR, str(i))
        tips = al.load_success_values_from_command_file(result_file, Tip)
        all_tips.extend(tips)
    return all_tips


def _solved_problems_from_summary_file(summary_path: Path) -> set[str]:
    """
    Load a summary CSV file and return the set of solved problems.
    """
    import pandas as pd  # type: ignore

    df = cast(Any, pd.read_csv(summary_path))  # type: ignore
    solved = df[df["success"]]["problem"]
    return set(solved)


#####
##### CLI
#####


class LearningExperimentCLI:
    def __init__(self, experiment: LearningExperiment):
        self.experiment = experiment

    def train(
        self,
        *,
        iter: int,
        max_workers: int = 1,
        retry_errors: bool = False,
        interactive: bool = False,
        force_summary: bool = False,
    ):
        """
        Solve training problems.
        """
        exp = self.experiment.training_experiment(iter)
        exp.load()
        if force_summary:
            exp.save_summary(ignore_missing=True)
            return
        if retry_errors:
            exp.mark_errors_as_todos()
        exp.resume(max_workers=max_workers, interactive=interactive)

    def feedback(
        self,
        *,
        iter: int,
        max_workers: int = 1,
    ):
        """
        Process training problem traces for producing feedback. This
        does not issue LLM queries.
        """
        self.experiment.generate_feedback_file(
            iter, max_workers=max_workers, show_progress=True
        )

    def analyze(
        self,
        *,
        iter: int,
        max_workers: int = 1,
        retry_errors: bool = False,
        interactive: bool = False,
    ):
        """
        Generate tips from feedback.
        """
        exp = self.experiment.generate_tips_experiment(iter)
        exp.load()
        if retry_errors:
            exp.mark_errors_as_todos()
        exp.resume(max_workers=max_workers, interactive=interactive)

    def summarize(
        self,
        *,
        iter: int,
        max_workers: int = 1,
        retry_errors: bool = False,
        interactive: bool = False,
    ):
        """
        Summarize all generated tips.
        """
        exp = self.experiment.summarize_tips_experiment(iter)
        exp.load()
        if retry_errors:
            exp.mark_errors_as_todos()
        exp.resume(max_workers=max_workers, interactive=interactive)

    def learn(self, *, iter: int):
        """
        Generate the learned data for a given iteration. This should be
        relatively fast.
        """
        self.experiment.generate_learned_data(iter)

    def test(
        self,
        *,
        iter: int,
        max_workers: int = 1,
        retry_errors: bool = False,
        interactive: bool = False,
        force_summary: bool = False,
    ):
        """
        Solve testing problems.
        """
        exp = self.experiment.testing_experiment(iter)
        exp.load()
        if force_summary:
            exp.save_summary(ignore_missing=True)
            return
        if retry_errors:
            exp.mark_errors_as_todos()
        exp.resume(max_workers=max_workers, interactive=interactive)

    def results(self, *, final_iter: int):
        """
        Print a summary of training and testing results up to a given
        iteration.
        """
        self.experiment.print_training_results(final_iter)
        self.experiment.print_testing_results(final_iter)


#####
##### Gathering Feedback
#####


@dataclass
class QueryFeedback:
    """
    Aggregated feedback for a specific query.
    """

    problem_name: str
    query: dp.SerializedQuery
    good_answers: list[dp.Answer]
    bad_answers: list[tuple[dp.Answer, dp.Error]]


type SerializedQueryFeedback = Any
"""
JSON representation of a `QueryFeedback` object.
"""


def gather_feedback_per_query(
    resolver: an.IRefResolver,
    *,
    problem_name: str,
    roots: Sequence[fp.NodeId],
    filter_sources: fp.FeedbackFilter | None = None,
    filter_backprop_handlers: fp.FeedbackFilter | None = None,
) -> Sequence[QueryFeedback]:
    """
    Extract aggregated feedback per query from the given roots and
    sources (see `process_feedback`).
    """
    feedback: dict[fp.AnswerId, QueryFeedback] = {}
    for f in fp.process_feedback(
        resolver,
        roots=roots,
        filter_sources=filter_sources,
        filter_backprop_handlers=filter_backprop_handlers,
    ):

        def unparse(v: Any) -> dp.Answer:
            ans = f.query.unparse(v)
            assert ans is not None, (
                f"Cannot unparse answer to {type(f.query.query_name())}: {v}"
            )
            return ans

        if f.answer_id not in feedback:
            serialized = dp.SerializedQuery.make(f.query)
            feedback[f.answer_id] = QueryFeedback(
                problem_name=problem_name,
                query=serialized,
                good_answers=[],
                bad_answers=[],
            )
        if isinstance(f.feedback, hf.GoodValue):
            feedback[f.answer_id].good_answers.append(f.answer)
        elif isinstance(f.feedback, hf.BadValue):
            feedback[f.answer_id].bad_answers.append(
                (f.answer, f.feedback.error)
            )
        elif isinstance(f.feedback, hf.BetterValue):
            assert isinstance(f.answer, dp.Answer)
            feedback[f.answer_id].good_answers.append(
                unparse(f.feedback.value)
            )
        elif isinstance(f.feedback, hf.BadValueAlso):
            feedback[f.answer_id].bad_answers.append(
                (unparse(f.feedback.value), f.feedback.error)
            )
    return list(feedback.values())


def feedback_from_command_file(
    command_file: Path,
    *,
    object_loader: dp.ObjectLoader,
    problem_name: str,
    enabled_feedback_nodes: Sequence[str],
) -> Sequence[QueryFeedback]:
    """
    Load query feedback from a given command file.
    """
    trace_data = al.load_trace_data_from_command_file(command_file)
    resolver = trace_data.resolver(object_loader)
    filter: fp.FeedbackFilter = (
        lambda label, node_id: label in enabled_feedback_nodes
    )
    return list(
        gather_feedback_per_query(
            resolver,
            problem_name=problem_name,
            roots=[fp.NodeId(i) for i in trace_data.success_nodes[:1]],
            filter_sources=filter,
            filter_backprop_handlers=filter,
        )
    )


def _process_single_problem_feedback(
    problem_name: str,
    command_file: Path,
    context: ec.ExecutionContext,
    enabled_feedback_nodes: Sequence[str],
) -> Sequence[QueryFeedback]:
    """
    Helper function to extract feedback from a single problem.
    This is a top-level function to support pickling for ProcessPoolExecutor.
    """
    object_loader = context.object_loader(extra_objects=None)
    return feedback_from_command_file(
        command_file,
        object_loader=object_loader,
        problem_name=problem_name,
        enabled_feedback_nodes=enabled_feedback_nodes,
    )


type RandomSelection = tuple[int, int]
"""
A (num_selected, among) pair where `num_selected` items are to be
randomly selected among `among` total items that maximize some
criterion.
"""


@dataclass
class FeedbackFilteringSettings:
    """
    Settings for filtering feedback.

    Attributes:
        max_per_problem: The number of feedback items to collect per
            problem, randomly selected among candidates with the maximal
            number of wrong answers (before the `max_wrong_answers`
            filter is applied).
        max_total: The total number of feedback items to collect,
            randomly selected among candidates with the maximal number
            after the `max_per_problem` filter is applied.
        max_wrong_answers: Maximum number of wrong answers per feedback
            item. If there are more, a random subset must be selected.
        ensure_one_good_answer_exactly: Discard items that have multiple
            correct answers and select the first one if there are multiple.
    """

    max_per_problem: RandomSelection
    max_total: RandomSelection
    max_wrong_answers: int
    ensure_one_good_answer_exactly: bool


type FeedbackFilteringSettingsDict = dict[str, FeedbackFilteringSettings]
"""
A mapping from query types to their feedback filtering settings.
"""


def filter_feedback(
    feedback: Sequence[QueryFeedback],
    *,
    rng: random.Random,
    settings: FeedbackFilteringSettingsDict,
) -> Sequence[QueryFeedback]:
    """
    Filter feedback according to the given settings.

    See `FeedbackFilteringSettings` for details.
    """

    # Group feedback items by query type (name)
    by_type: dict[str, list[QueryFeedback]] = {}
    for fb in feedback:
        qtype = fb.query.name
        if qtype not in settings:
            raise ValueError(f"No filter settings for query type '{qtype}'")
        by_type.setdefault(qtype, []).append(fb)

    def rank_wrong_count(item: QueryFeedback) -> int:
        # Ranking metric: number of wrong answers BEFORE any trimming
        return len(item.bad_answers)

    def select_top_then_sample(
        items: Sequence[QueryFeedback], sel: RandomSelection
    ) -> list[QueryFeedback]:
        num_selected, among = sel
        assert 0 <= num_selected <= among
        if num_selected == 0:
            return []
        if len(items) <= num_selected:
            return list(items)
        sorted_items = sorted(items, key=rank_wrong_count, reverse=True)
        pool = sorted_items[:among]
        return rng.sample(pool, num_selected)

    result: list[QueryFeedback] = []

    for qtype, items in by_type.items():
        cfg = settings[qtype]

        # 0) If ensure_one_good_answer_exactly is set, filter and validate
        if cfg.ensure_one_good_answer_exactly:
            filtered_items: list[QueryFeedback] = []
            for it in items:
                if len(it.good_answers) == 0:
                    # Drop items with no good answers
                    continue
                elif len(it.good_answers) > 1:
                    # Select the first good answer only
                    it = replace(it, good_answers=[it.good_answers[0]])
                    filtered_items.append(it)
                else:
                    filtered_items.append(it)
            items = filtered_items

        # 1) Per-problem selection
        by_problem: dict[str, list[QueryFeedback]] = {}
        for it in items:
            by_problem.setdefault(it.problem_name, []).append(it)
        per_problem_selected: list[QueryFeedback] = []
        for _, prob_items in by_problem.items():
            per_problem_selected.extend(
                select_top_then_sample(prob_items, cfg.max_per_problem)
            )
        if not per_problem_selected:
            continue

        # 2) Global selection across all problems for this query type
        globally_selected = select_top_then_sample(
            per_problem_selected, cfg.max_total
        )

        # 3) Apply per-item constraints and produce copies
        for it in globally_selected:
            # Enforce max_wrong_answers by subsampling wrong answers
            bad = it.bad_answers
            if len(bad) > cfg.max_wrong_answers:
                bad = rng.sample(bad, cfg.max_wrong_answers)
            # Append a fresh QueryFeedback with trimmed bad answers
            result.append(
                QueryFeedback(
                    problem_name=it.problem_name,
                    query=it.query,
                    good_answers=it.good_answers,
                    bad_answers=bad,
                )
            )
    return result


#####
##### Learned Data
#####


class Tip(TypedDict):
    """
    A tip for how to answer a query, which can be stored in a data file.
    """

    name: str
    content: str


def generate_demonstrations(feedback: QueryFeedback) -> dm.QueryDemo | None:
    """
    Generate a positive demonstration from the given query feedback.
    """
    if feedback.good_answers == []:
        return None
    answers = [
        replace(
            dm.reverse_translate_answer(ans),
            meta={"problem": feedback.problem_name},
        )
        for ans in feedback.good_answers
    ]
    return dm.QueryDemo(
        query=feedback.query.name,
        args=feedback.query.args_dict,
        answers=answers,
    )


def write_demonstration_file(
    feedback: Sequence[QueryFeedback], path: Path
) -> None:
    """
    Write a demonstration file from the given query feedback.

    This function generates query demonstrations from feedback entries
    and writes them to a YAML file. Parent directories are created if
    they don't exist.
    """
    demos: list[dm.QueryDemo] = []
    for fb in feedback:
        demo = generate_demonstrations(fb)
        if demo is not None:
            demos.append(demo)
    path.parent.mkdir(parents=True, exist_ok=True)
    demo_file_content = dump_yaml(dm.DemoFile, demos, exclude_defaults=True)
    path.write_text(demo_file_content)
